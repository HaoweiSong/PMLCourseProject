---
title: "PMLweek4CourseProject"
author: "Haowei Song"
date: "September 14, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Using data from from this source: http://groupware.les.inf.puc-rio.br/har, the goal of this project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. Any of the other variables could be used to predict with. Different machine learning model need to be built and tested with cross validation. Sample error will also be estimated. At last, the best prediction model will be used to predict 20 different test cases.

## Data Input and Exploratory Analysis
```{r load libraries, echo=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)
```
### Read the training and testing data set
```{r Data}
train_data <- read.csv("pml-training.csv")
test_data <- read.csv("pml-testing.csv")
```

### Exploratory Analysis
```{r Data exploratory}
#Training data set
colnames(train_data)
#Testing data set
colnames(test_data)
```

##Data Processing

### Cleaning the training and testing data set
####In this section, removing those variables with nearly zero variance, variables that are almost always NA, and variables that don't make intuitive sense for prediction.

```{r Data Cleaning}
# Removing variables with nearly zero variance
nzv <- nearZeroVar(train_data)
train_data <- train_data[, -nzv]
test_data <- test_data[, -nzv]

# Removing variables that are almost always NA
na <- sapply(train_data, function(x) mean(is.na(x))) > 0.95
train_data <- train_data[, na==F]
test_data <- test_data[, na==F]

# removing variables (X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp) that is not useful for prediction
train_data <- train_data[, -(1:5)]
test_data <- test_data[, -(1:5)]
```
###Data Preparation
####In this section, training data set will be splited into a smaller training data set and a validation data set.
```{r Create validation set}
inTrain<-createDataPartition(y=train_data$classe, p=0.7, list=F)
trainingSmall <- train_data[inTrain, ]
validation <- train_data[-inTrain, ]
```

## Machine Learning Models  

###Classification Tree

#### Building the model
```{r Create Decision Trees }
set.seed(12345)
modFit_tree <- rpart(classe ~ ., data=trainingSmall, method="class")
fancyRpartPlot(modFit_tree)
```

#### Cross Validation and Out of sample error
```{r Cross validation with the splited data set }
modFit_tree_validation <- predict(modFit_tree, validation, type = "class")
Validation_tree <- confusionMatrix(modFit_tree_validation, validation$classe)
Validation_tree
```

#### Building the model
###Random Forest
```{r Create RF model }
set.seed(123)
modFit_rf <- randomForest(classe ~ ., data=trainingSmall)
modFit_rf
```
#### Cross Validation and Out of sample error
```{r Cross validation for the rf model}
modFit_rf_validation <- predict(modFit_rf, validation, type = "class")
Validation_rf <- confusionMatrix(modFit_rf_validation, validation$classe)
Validation_rf
```
## Prediction
### Model random forest has better accuracy than decision tree, therefore, we are using random forest to predict the result of the testing data set. 
```{r Predict the testing data set with the rf model}
prediction_rf_test_data <- predict(modFit_rf, test_data, type = "class")
prediction_rf_test_data
```

